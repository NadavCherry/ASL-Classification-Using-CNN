{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import vutils\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,log_loss\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.catch_warnings()\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import lightning as L\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor,Lambda\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import neptune.new as neptune\n",
    "from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import pytorch_lightning as pl\n",
    "import os\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e52bf777ddc0cf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e588488a4012cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44b1365dfffa37d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "test_data_path = 'archive/test'\n",
    "# train_path = 'C:\\\\Users\\\\nadav\\\\PycharmProjects\\\\assignment1\\\\pythonProject7\\\\archive\\\\asl_alphabet_subset'\n",
    "train_path = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n",
    "\n",
    "label_mapping = {\n",
    "        'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9,\n",
    "        'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18,\n",
    "        'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26,\n",
    "        'nothing': 27, 'space': 28, 'five': 29\n",
    "    } \n",
    "inverted_label_mapping = {v: k for k, v in label_mapping.items()}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df8da4488ec8ec82",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "data_path = './data_folder/'\n",
    "\n",
    "class CNN_ASL(pl.LightningModule):\n",
    "    def __init__(self, train_data, val_data, data_dir=data_path, num_classes=29, learning_rate=2e-4, batch_size=batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.run = neptune.init_run(\n",
    "        project=\"nadavcherry/dp1\",\n",
    "        api_token=\"a\", # your credentials\n",
    "        )\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((200, 200)),\n",
    "                # transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.asl_test = datasets.ImageFolder(test_data_path, transform=transform)\n",
    "        self.asl_train = train_data\n",
    "        self.asl_val = val_data\n",
    "        self.data_dir = data_dir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size=batch_size\n",
    "        self.y = []\n",
    "        self.preds = []\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ## input channels 1 - monochrom (for rgb would be 3)\n",
    "        ## output channels 32 - as the number of filters that we train\n",
    "        ## kernel size 3 - arbitrary selection\n",
    "        self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n",
    "        self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n",
    "        self.conv1 = nn.Conv2d(32,64,3,padding='same') \n",
    "        ## input channels 1 - monochrom (for rgb would be 3)\n",
    "        ## output channels 32 - as the number of filters that we train\n",
    "        ## kernel size 3 - arbitrary selection\n",
    "        # self.conv1 = nn.Conv2d(1,32,3,padding='same')\n",
    "        self.conv_11 = nn.Conv2d(1,16,11, padding='same')\n",
    "        self.conv_7 = nn.Conv2d(16,32,7, padding='same')\n",
    "        self.conv1 = nn.Conv2d(32,64,3,padding='same') \n",
    "        self.conv2 = nn.Conv2d(64,128,3,padding='same')\n",
    "        self.conv3 = nn.Conv2d(128,64,3,padding='same')\n",
    "        self.conv4 = nn.Conv2d(64,64,3,padding='same')\n",
    "        self.linear1 = nn.Linear(25*25*64,50)\n",
    "        self.linear2 = nn.Linear(50,self.num_classes)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.count_batch_train = 0\n",
    "        self.count_batch_val = 0\n",
    "        self.count_batch_test = 0\n",
    "        self.train_loss1 = 0\n",
    "        self.train_accuracy1 = 0\n",
    "\n",
    "        self.val_loss1 = 0\n",
    "        self.val_accuracy1 = 0\n",
    "\n",
    "        self.test_loss1 = 0\n",
    "        self.test_accuracy1 = 0\n",
    "        \n",
    "        self.val_loss_arr = []\n",
    "        self.val_accuracy_arr = []\n",
    "        \n",
    "        self.count_good_high_confidence = 0\n",
    "        self.count_good_classification_uncertain_confidence = 0\n",
    "        self.count_bad_classification = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv_11(x))\n",
    "        x = self.relu(self.conv_7(x))\n",
    "        x = self.mp(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.mp(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.mp(x)\n",
    "        x = x.view(-1,25*25*64) ### WHY -1\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_accuracy(preds, y)\n",
    "        acc1 = acc.item()\n",
    "        loss1 = loss.item()\n",
    "        self.train_accuracy1 += acc1\n",
    "        self.train_loss1 += loss1\n",
    "        self.count_batch_train += 1\n",
    "        self.run[\"train/accuracy_batch\"].log(acc1)\n",
    "        self.run[\"train/loss_batch\"].log(loss1)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.run[\"train/accuracy_epochs\"].log(self.train_accuracy1/self.count_batch_train)\n",
    "        self.run[\"train/loss_epochs\"].log(self.train_loss1/self.count_batch_train)\n",
    "        self.train_accuracy1 = 0\n",
    "        self.train_loss1 = 0\n",
    "        self.count_batch_train = 0\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_accuracy(preds, y)\n",
    "        acc1 = acc.item()\n",
    "        loss1 = loss.item()\n",
    "        self.val_accuracy1 += acc1\n",
    "        self.val_loss1 += loss1\n",
    "        self.count_batch_val += 1\n",
    "        self.run[\"val/accuracy_batch\"].log(acc1)\n",
    "        self.run[\"val/loss_batch\"].log(loss1)\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val/val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/val_acc\", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_acc = self.val_accuracy1/self.count_batch_val\n",
    "        val_loss_temp = self.val_loss1/self.count_batch_val\n",
    "        self.run[\"val/accuracy_epochs\"].log(val_acc)\n",
    "        self.run[\"val/loss_epochs\"].log(val_loss_temp)\n",
    "        \n",
    "        self.val_loss_arr.append(val_loss_temp)\n",
    "        self.val_accuracy_arr.append(val_acc)\n",
    "        \n",
    "        self.val_accuracy1 = 0\n",
    "        self.val_loss1 = 0\n",
    "        self.count_batch_val = 0\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        logits = logits.cpu()\n",
    "        y = y.cpu()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "       # Compute softmax probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        acc = self.test_accuracy(preds, y)\n",
    "        self.test_accuracy1 += acc.item()\n",
    "        self.test_loss1 += loss.item()\n",
    "        self.count_batch_test += 1\n",
    "        self.count_good_high_confidence = 0\n",
    "        self.count_good_classification_uncertain_confidence = 0\n",
    "        transform1 = transforms.ToPILImage()\n",
    "        for i in range(len(y)):    \n",
    "            self.y.append(y[i])\n",
    "            self.preds.append(preds[i])\n",
    "            if y[i] == preds[i]:\n",
    "                if probs[i, preds[i]] > 0.95:  # High confidence good classification\n",
    "                    if self.count_good_high_confidence == 2:\n",
    "                        continue\n",
    "                    self.count_good_high_confidence += 1\n",
    "                    img_path = f\"images/example_good_high_confidence_{batch_idx}_{i}.png\"                \n",
    "                    transform1(x[i]).save(img_path)\n",
    "                    self.run[f\"Good classification High confidence/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n",
    "                    # display(transform1(x[i]))\n",
    "                elif probs[i, preds[i]] < 0.4:  # Uncertain classification\n",
    "                    if self.count_good_classification_uncertain_confidence == 2:\n",
    "                        continue\n",
    "                    self.count_good_classification_uncertain_confidence += 1\n",
    "                    img_path = f\"images/Uncertain_confidence_{batch_idx}_{i}.png\"\n",
    "                    transform1(x[i]).save(img_path)\n",
    "                    self.run[f\"Good classification Uncertain classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n",
    "            elif probs[i, preds[i]] > 0.4:\n",
    "                self.count_bad_classification += 1\n",
    "                # if self.count_bad_classification > 100:\n",
    "                #     continue\n",
    "                img_path = f\"images/Bad_classification_{batch_idx}_{i}.png\"\n",
    "                transform1(x[i]).save(img_path)\n",
    "                self.run[f\"Bad_classification/true label: {inverted_label_mapping[y[i].item()]}, prediction: {inverted_label_mapping[preds[i].item()]}, probabilities: {probs[i, preds[i]]}, Batch_id: {batch_idx}, Example: {i}\"].upload(img_path)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_end(self):\n",
    "        cm = confusion_matrix(self.y, self.preds)\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, cmap='Greens', annot=True, fmt='d')\n",
    "        plt.xlabel('Prediction')\n",
    "        plt.ylabel('True label')\n",
    "        plt.title('ASL Convolutional Model\\nClassification Results on Test Set')\n",
    "        \n",
    "        # Save the confusion matrix plot\n",
    "        cm_plot_path = 'confusion_matrix_plot.png'\n",
    "        plt.savefig(cm_plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        folder_path = \"lightning_logs\"\n",
    "        version = os.listdir(folder_path)[-1] + '/checkpoints'\n",
    "        file_path = os.listdir(folder_path+'/'+version)\n",
    "        f = str(folder_path+'/'+version+'/'+file_path[0])\n",
    "        self.run[f'Confusion_Matrix_Plot'].upload(cm_plot_path)\n",
    "        self.run[\"test_accuracy\"] = (self.test_accuracy1/self.count_batch_test)\n",
    "        self.run[\"test_loss\"] = (self.test_loss1/self.count_batch_test)\n",
    "        self.run[f\"{file_path[0]}\"].upload(f)    \n",
    "        self.run.stop()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.asl_train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.asl_val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.asl_test, batch_size=self.batch_size)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "636b3ee2f8eb1007",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((200, 200)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)\n",
    "dataset_size = len(train_dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = dataset_size - train_size\n",
    "train_data, train_val = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader  = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(train_val, batch_size=batch_size)\n",
    "# \n",
    "kf_model = CNN_ASL(train_loader, val_loader, num_classes=30)\n",
    "# Define EarlyStopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val/val_loss',  # Metric to monitor for improvement\n",
    "    min_delta=0.001,      # Minimum change in the monitored metric to qualify as improvement\n",
    "    patience=3,           # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,         # Print message when training is stopped due to early stopping\n",
    "    mode='min'            # 'min' or 'max': whether the monitored metric should be minimized or maximized\n",
    ")\n",
    "\n",
    "# Initialize Trainer with EarlyStopping callback\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=50,\n",
    "    callbacks=[early_stop_callback]  # Pass the EarlyStopping callback to the Trainer\n",
    ")\n",
    "trainer.fit(kf_model)\n",
    "# # \n",
    "# # \n",
    "# # # \n",
    "# # # model = CNN_ASL.load_from_checkpoint(\"lightning_logs/version_181/checkpoints/epoch=19-step=19580.ckpt\",train_data=train_data, val_data=train_val)\n",
    "# # # # model = CNN_ASL.load_from_checkpoint(\"lightning_logs/version_283/checkpoints/epoch=4-step=95.ckpt\", train_data=train_data, val_data=train_val)\n",
    "# # # # disable randomness, dropout, etc...\n",
    "# # # model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a5708c4db4bf40",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.test(kf_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ccd18fee8bcd8f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad9566c3b5632b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((200, 200)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3beb9188dd93701b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = datasets.ImageFolder(train_path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433fcca75369a839",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "max_acc = 0\n",
    "fold_num = 5\n",
    "ensemble_models = []\n",
    "test_accuracy = []\n",
    "test_loss = []\n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "kf = StratifiedKFold(n_splits=fold_num, shuffle=True)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(train_dataset,train_dataset.targets)):\n",
    "\n",
    "\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_index)\n",
    "    val_subset = torch.utils.data.Subset(train_dataset, val_index)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    kf_model = CNN_ASL(train_loader, val_loader, num_classes=30)\n",
    "    # Define EarlyStopping callback\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val/val_loss',  # Metric to monitor for improvement\n",
    "        min_delta=0.001,      # Minimum change in the monitored metric to qualify as improvement\n",
    "        patience=3,           # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=True,         # Print message when training is stopped due to early stopping\n",
    "        mode='min'            # 'min' or 'max': whether the monitored metric should be minimized or maximized\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with EarlyStopping callback\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=50,\n",
    "        callbacks=[early_stop_callback]  # Pass the EarlyStopping callback to the Trainer\n",
    "    )\n",
    "    trainer.fit(kf_model)\n",
    "    trainer.test(kf_model)\n",
    "\n",
    "    ensemble_models.append(kf_model)\n",
    "    # Calculate test accuracy and test loss\n",
    "\n",
    "    test_accuracy.append(kf_model.test_accuracy1/kf_model.count_batch_test)\n",
    "    test_loss.append(kf_model.test_loss1/kf_model.count_batch_test)\n",
    "    val_accuracy.append(kf_model.val_accuracy_arr)\n",
    "    val_loss.append(kf_model.val_loss_arr)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a927dc57ac55fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "kf_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5fcadb1ad976b8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ensemble_models.append(kf_model)\n",
    "# Calculate test accuracy and test loss\n",
    "test_accuracy = []\n",
    "test_loss = []\n",
    "val_accuracy = []\n",
    "val_loss = []\n",
    "for i in range(5):\n",
    "    kf_model = ensemble_models[i]\n",
    "    test_accuracy.append(kf_model.test_accuracy1/kf_model.count_batch_test)\n",
    "    test_loss.append(kf_model.test_loss1/kf_model.count_batch_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c78815ddd9cb7734",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(val_accuracy[0]))\n",
    "print(len(val_accuracy[1]))\n",
    "print(len(val_accuracy[2]))\n",
    "print(len(val_accuracy[3]))\n",
    "print(len(val_accuracy[4]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ef082f615c383",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming you have lists for validation and test losses and accuracies\n",
    "epochs = range(0,8)\n",
    "\n",
    "# Create two subplots (one for loss and one for accuracy)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 8))\n",
    "\n",
    "column_averages_loss = [sum(col) / len(col) for col in zip(*val_loss)]\n",
    "column_averages_acc = [sum(col) / len(col) for col in zip(*val_accuracy)]\n",
    "\n",
    "# Plot the training and validation losses\n",
    "ax1.plot(epochs, column_averages_loss, label='Validation Loss', marker='o', linestyle='-')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Average Validation Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the training and validation accuracies\n",
    "ax2.plot(epochs, column_averages_acc, label='Validation Accuracy', marker='s', linestyle='--', color='orange')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Average Validation Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cd68a79317257af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming you have test_accuracy and test_loss lists containing values for each fold\n",
    "\n",
    "fold_num = len(test_accuracy)\n",
    "\n",
    "# Create a list of lists to store data for the table\n",
    "table_data = [[\"Fold\", \"Test Accuracy\", \"Test Loss\"]]\n",
    "for i in range(fold_num):\n",
    "    table_data.append([i+1, test_accuracy[i], test_loss[i]])\n",
    "\n",
    "# Calculate average test accuracy and test loss\n",
    "avg_test_accuracy = sum(test_accuracy) / fold_num\n",
    "avg_test_loss = sum(test_loss) / fold_num\n",
    "\n",
    "# Add rows for average values\n",
    "table_data.append([\"Average\", avg_test_accuracy, avg_test_loss])\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16f570461ee21114",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets, utils as tv_utils\n",
    "\n",
    "class RandomTurnOffPixels(object):\n",
    "    def __init__(self, probability=0.1):\n",
    "        self.probability = probability\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.probability:\n",
    "            img = img.convert(\"RGB\")  # Convert to RGB if not already\n",
    "            img_array = img.load()\n",
    "            width, height = img.size\n",
    "            num_pixels_to_turn_off = int(self.probability * width * height)\n",
    "            for _ in range(num_pixels_to_turn_off):\n",
    "                x = random.randint(0, width - 1)\n",
    "                y = random.randint(0, height - 1)\n",
    "                img_array[x, y] = (0, 0, 0)  # Set pixel to black\n",
    "            return img\n",
    "        return img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54780fb3d4dff99f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "accuracy1 = kf_model.test_accuracy1/kf_model.count_batch_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f6ec632a8dc74f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print('Accuracy without augmentations:', accuracy1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf3421b074f1caf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "kf_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46da206ee1e35163",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "to_pseudo_rgb = Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "test_dataloader = DataLoader(datasets.ImageFolder(test_data_path, transform=transform), batch_size=64)\n",
    "augmentations = [\n",
    "    transforms.Compose([  \n",
    "        transforms.ToPILImage(),\n",
    "        RandomTurnOffPixels(probability=0.2),\n",
    "        transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "\n",
    "    transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "    ])            \n",
    "\n",
    "]\n",
    "x = 0\n",
    "# Iterate over the test dataset\n",
    "for images, labels in test_dataloader:        \n",
    "    # Initialize final predictions\n",
    "    final_predictions = torch.zeros(len(labels), 29)\n",
    "\n",
    "    # Iterate for the specified number of prediction runs\n",
    "    for j in range(2):\n",
    "        # Perform predictions\n",
    "        \n",
    "        aug_index = j % len(augmentations)\n",
    "        with torch.no_grad():\n",
    "            aug_images = torch.stack([augmentations[aug_index](image) for image in images])\n",
    "            preds = kf_model(aug_images)\n",
    "\n",
    "        # Accumulate predictions\n",
    "        final_predictions += preds\n",
    "        if x == 0:\n",
    "            x += 1\n",
    "    # Average predictions\n",
    "    final_predictions /= 2\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predicted_labels = torch.argmax(final_predictions, dim=1)\n",
    "    \n",
    "    correct_predictions += (predicted_labels == labels).sum().item()\n",
    "    total_predictions += len(labels)\n",
    "    \n",
    "    # Display the augmented images\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(\"Accuracy:\",accuracy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6bdf76c9c5e53a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2d8858bf745578ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12f6dc2936eabf98",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for model in ensemble_models:\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         predictions = []\n",
    "#         for inputs, labels in test_loader:\n",
    "#             outputs = model(inputs)\n",
    "#             predictions.extend(torch.argmax(outputs, dim=1).cpu().tolist())\n",
    "#     individual_predictions.append(predictions)\n",
    "# individual_predictions = [[0,1,0,3,2,1],[3,3,2,0,1,5],[3,1,5,2,3,4],[1,2,3,4,5,4]]\n",
    "# \n",
    "# common_predictions = set(individual_predictions[0]).intersection(*individual_predictions[1:])\n",
    "# \n",
    "# # Convert common predictions back to a list if needed\n",
    "# common_predictions = list(common_predictions)\n",
    "# \n",
    "# print(\"Common predictions:\", common_predictions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d96f8aaa9f0f06ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# \n",
    "# # Define the directory containing the images\n",
    "# image_dir = 'C:\\\\Users\\\\nadav\\\\Downloads\\\\Final_Data_Full\\\\Final_Data_Full\\\\train'\n",
    "# \n",
    "# # Get a list of all image files in the directory\n",
    "# image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "# \n",
    "# # Iterate over each image file\n",
    "# for image_file in image_files:\n",
    "#     # Extract the prefix of the image file name up to the '_' character\n",
    "#     prefix = image_file.split('_')[0]\n",
    "#     \n",
    "#     # Create directory if it doesn't exist\n",
    "#     target_dir = os.path.join(image_dir, prefix)\n",
    "#     if not os.path.exists(target_dir):\n",
    "#         os.makedirs(target_dir)\n",
    "#     \n",
    "#     # Move the image file to the corresponding directory\n",
    "#     src_path = os.path.join(image_dir, image_file)\n",
    "#     dst_path = os.path.join(target_dir, image_file)\n",
    "#     shutil.move(src_path, dst_path)\n",
    "# \n",
    "# print(\"Images moved to appropriate folders.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc4fdd942e2febb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# model = CNN_ASL.load_from_checkpoint(\"lightning_logs/version_181/checkpoints/epoch=19-step=19580.ckpt\",train_data=train_data, val_data=train_val)\n",
    "# # model = CNN_ASL.load_from_checkpoint(\"lightning_logs/version_283/checkpoints/epoch=4-step=95.ckpt\", train_data=train_data, val_data=train_val)\n",
    "# # disable randomness, dropout, etc...\n",
    "# model.eval()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5521e914fc4510d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
